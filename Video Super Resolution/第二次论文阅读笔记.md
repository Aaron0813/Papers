___Video Super-Resolution via Deep Draft-Ensemble Learning___
===

*概述*
    
    提出一个新的fast video super resolution方法，具体包括两个阶段，即SR draft ensemble的生成过程和它的最优化重建过程
    
    SR draft ensemble: 对传统前向传递重建方法的革新，增强了在大幅度变化与较高错
    误率下对于不同SR结果的计算能力
    
    重建：通过CNN完成

*创新点与优势*
    
    1. SR draft ensemble的生成过程：quickly produces a set of SR drafts from the input LR sequence。降低计算量
    
    2. CNN将会接收到更多的纹理信息：使用了discriminative learning。其他方法都是在最后产生阶段才会开始寻找最优解，但是本论文会根据第一阶段产生的众多draft中寻找一个最优解

*实现细节*
    
    1. draft产生阶段：以前，是通过一个非常复杂但是又效果不是那么好的数学操作产生draft的，公式如下图所示。


![公式][07]
[07]: https://github.com/Aaron0813/Pictures/blob/master/%E5%85%AC%E5%BC%8F4.PNG?raw=true "原公式"

    但事实上，这个公式的参数非常复杂，以至于在本论文以前的程序很大一部分计算量都消耗在这里。
    于是作者就想，能不能找一个更快的算法呢？然后他们就采取了一个更加快速的算法。如下图所示。

![公式][08]
[08]: https://github.com/Aaron0813/Pictures/blob/master/%E7%AE%97%E6%B3%951.PNG?raw=true "新算法"

    至于这个算法是什么涵义，恕在下才疏学浅，这个真的没看懂，一堆转置绕后相乘，我根本看不懂想要做什么，但是就是达到目的了-快速产生draft

    需要补充的一点是，这个算法不一定能产生出同等于后期算法所生成出来图片的质量
    ，因为其利用的信息相对来说还是少了点。为此 作者在生成draft的时候加入了a
    set of motion estimates（用了两个光流算法）。实验结果表明确实有效，同时对于不同图片，其最优结果所依赖的光流算法的参数还可能是不一样的。（这点还是比较好理解的）

    2.通过CNN找出最优或者接近最优的SR draft（为什么说接近最优呢？因为有的时候数据上的一个draft可能是最优的，但是对于人眼来说可能不是那么的好）

    至于网络的结构上没有什么革新，中规中矩的CNN，不过loss函数中加入了TV 正则化

    3.deconvolution


*实验结果*
    
    数据上有提升，但个人觉得最显著的是在速度上。论文中说Bayesian Video SR得用两个小时才能得出结果，但本文只要大约500s（这个我跑了一下速度确实差不多，运行几次都是520s左右）

*可能的改进与展望*
    
    1. 本CNN用的是tanh,一般来说是relu跑起来速度更快同时效果差不多，或许考虑可以在这块加以改进。但是也不绝对，比方说下一篇论文的测试结果显示就是tanh效果更好（我没亲测，感觉这个网络训练得好几天，但是我们学校每天晚上断电。。。）

    2. 不知道后期有没有机会把draft的产生阶段也集成到网络中去（我这么说，纯属是站着说话不腰疼，因为感觉这个还是很有难度的）



___Real-Time Single Image and Video Super-Resolution Using an Efﬁcient  Sub-Pixel Convolutional Neural Network___
===

*概述*
    
    常见的SR问题解决方案中，通常会将LR图片通过bicubic放大到HR空间，然后进行图像
    的重建操作。然而经过验证可知，这样的操作不是最优的，增加了计算量
    
    为此，作者提出了一种新颖的网络架构，该网络将直接在LR的图像中进行特征的提取(即不需要进行图像放大的预处理操作)
    ，而在最后的输出层将图像进行放大。这样将会减少网络前L-1层中的卷积计算量(如果网络层数为L)。测验表明图像质量有提升，速度提升感人，进而使对视频的实时
    处理变得更加有可能性(0.038s/frame,而SRCNN是0.435s--我之前测试数据是真的有问题，GPU没启动起来)。



*具体实现*

    训练过程：HR图像先高斯滤波，然后进行下采样得到LR图像。将LR图像通过三层网络进而学习。
    测试过程：输入一个LR图像，不进行放大操作直接输入卷积神经网络，通过两个卷积
    层以后，得到的特征图像大小与输入图像一样，但是特征通道为r^2(r是图像的目标放大倍数)。将每个像素的r^2个通道重新排列成一个r x 
    r的区域，对应于高分辨率图像中的一个r x r大小的子块，从而大小为r^2 x H x 
    W的特征图像被重新排列成1 x rH x rW大小的高分辨率图像
    

![网络结构][01]
[01]: https://syncedreview.files.wordpress.com/2017/04/46.png?w=1024 "网络结构"
*创新点与优势*
    
    1. 采用了sub-pixel convolutional layer的设计，降低了计算量，有效的对算法进行了加速(
    只在最后一层对图像大小做变换，前面的卷积运算由于在低分辨率图像上进行)

    2. 图像从低分辨率到高分辨率放大的过程，插值函数被隐含地包含在前面的卷积层中，可以自动学习到


*理解难点*

    sub-pixel convolutional layer的那个操作，数学公式有点复杂，看起来比较费劲。
    首先祭上第一个公式（下图）。
![公式][02]
[02]: https://raw.githubusercontent.com/Aaron0813/Pictures/master/%E5%85%AC%E5%BC%8F1.PNG?token=AQoo_KuvPRwmY921srnvUYFOBuZQICdAks5Z04m3wA%3D%3D "公式1"

    其实这个还是比较好理解的，描述的就是最后一层要做的事情：经过前两层的卷积操
    作之后，其要通过一个PS操作（就是f函数），将得到的众多LR图像合成一个图像。

    比较让人无语的是第二个公式，重点描述这个PS操作，如下图。
![公式][03]
[03]: https://github.com/Aaron0813/Pictures/blob/master/%E5%85%AC%E5%BC%8F2.PNG?raw=true "公式2"

    恕在下无能，我大眼瞪小眼看了半天也没看明白具体是个怎么样的操作。但是在
[Github](https://github.com/drakelevy/ESPCN-TensorFlow)
    
    上找到了别人用Tensorflow实现的代码（ESPCN.py里面），找到了最后一层的操作代
    码。研读了代码之后，感觉理解了不少，先乘255（这个乘操作没看懂，是对数据进行统一化处理吗？），然后转类型，分别取个最大值最小值（有点像Relu操作），然后
    转成正常图片形式，输出

    def generate(self, lr_image):
        lr_image = self.preprocess([lr_image, None])[0]
        sr_image = self.create_network(lr_image)
        sr_image = sr_image * 255.0
        sr_image = tf.cast(sr_image, tf.int32)
        sr_image = tf.maximum(sr_image, 0)
        sr_image = tf.minimum(sr_image, 255)
        sr_image = tf.cast(sr_image, tf.uint8)
        return sr_image
---
    但在
[另一个网站](https://syncedreview.com/2017/04/16/real-time-single-image-and-video-super-resolution-using-an-efficient-sub-pixel-convolutional-neural-network/)
    
    上看到了不同的操作，乍看之下感觉更像公式所表达的意思，但是无奈我公式看不懂，所以我也不知道哪个更贴切。
![公式][05]
[05]: https://syncedreview.files.wordpress.com/2017/04/e68d95e88eb7.png "具体实现"

---
    最后一层的Loss Function看公式可能有点懵，但其实也就是一个差平方求和求平均。

![公式][04]
[04]: https://github.com/Aaron0813/Pictures/blob/master/%E5%85%AC%E5%BC%8F3.PNG?raw=true "公式3"

    一并把代码贴出来好了。
    def loss(self, output, input_labels):
        residual = output - input_labels
        loss = tf.square(residual)
        reduced_loss = tf.reduce_mean(loss)
        tf.summary.scalar('loss', reduced_loss)
        return reduced_loss

*实验结果*

    总的来说，图片的质量上有些许提升，但感觉不是那么重要，重要的是速度提升（尤
    其是对视频的处理上，一下子快了好几个数量级），这样使对视频的实时处理成为了可能。
![速度][06]
[06]: https://syncedreview.files.wordpress.com/2017/04/113.png "速度比较"

    这个代码我没跑起来，一开始在网上找作者公开的代码，但是没找到（谷歌学术定位
    到的那个没有公开的project）。github找了一些别人自己实现的，matlab+caffe的那
    个因为没装caffe就没测试了，python的运行过程中出错，暂时没反应过来错在哪（为
    什么代码总是会跑不起来，看着代码眼馋，就是跑不动）







